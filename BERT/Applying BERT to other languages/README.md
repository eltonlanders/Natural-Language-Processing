# BERTje for Dutch 
<ul>
  <li> BERTje is the pre-trained monolingual BERT model for the Dutch language from the University of Groningen <br> </li>
  <li> The BERTje model is pre-trained using MLM and sentence order prediction (SOP) tasks with whole word masking (WWM). <br> </li>
  <li> The model has been pre-trained for about 1 million iterations <br> </li>
</ul>
  
